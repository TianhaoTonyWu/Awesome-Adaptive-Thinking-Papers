# Awesome-Adaptive-Thinking-Papers

---

# ðŸ“š Adaptive Thinking in LLMs

This repository curates research papers on adaptive thinking in large language models (LLMs).

---

## ðŸ”¹ Model Learns When to Think

### RL

* **AdaptThink: Reasoning models can learn when to think**  
  *Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, Juanzi Li*  
  2025.  
  [ðŸ“„ arXiv:2505.13417](https://arxiv.org/abs/2505.13417)

* **Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL**  
  *Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao*  
  2025.  
  [ðŸ“„ arXiv:2505.10832](https://arxiv.org/abs/2505.10832)

* **Think when you need: Self-adaptive chain-of-thought learning**  
  *Jiaqi Yang, Kaixuan Lin, Xinyu Yu*  
  2025.  
  [ðŸ“„ arXiv:2504.03234](https://arxiv.org/abs/2504.03234)

### SFT + RL

* **ThinkLess: LLM learns when to think**  
  *Gongfan Fang, Xiaotian Ma, Xinyu Wang*  
  2025.  
  [ðŸ“„ arXiv:2505.13379](https://arxiv.org/abs/2505.13379)

* **Think only when you need with large hybrid-reasoning models**  
  *Linyang Jiang, Xiaoxuan Wu, Shilin Huang, Qi Dong, Zhiqiang Chi, Li Dong, Xiaodong Zhang, Tian Lv, Lijuan Cui, Furu Wei*  
  2025.  
  [ðŸ“„ arXiv:2505.14631](https://arxiv.org/abs/2505.14631)



  
---


## ðŸ”¹ Routing

* **Arch-Router: Aligning LLM routing with human preferences**  
  *Co Tran, Salman Paracha, Adil Hafeez, Shuguang Chen*  
  2025.  
  [ðŸ“„ arXiv:2506.16655](https://arxiv.org/abs/2506.16655)

---


## ðŸ”¹ Long-to-Short CoT Baselines

* **L1: Controlling how long a reasoning model thinks with reinforcement learning**  
  *Pranjal Aggarwal, Sean Welleck*  
  2025.  
  [ðŸ“„ arXiv:2503.04697](https://arxiv.org/abs/2503.04697)

* **s1: Simple test-time scaling**   
  *Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, Tatsunori Hashimoto*   
  2025.  
  [ðŸ“„ arXiv:2501.19393](https://arxiv.org/abs/2501.19393)

* **Reasoning models can be effective without thinking**  
  *Weizhi Ma, Jinhyuk He, Charles Snell, Thomas Griggs, Sewon Min, Matei Zaharia*  
  2025.  
  [ðŸ“„ arXiv:2504.09858](https://arxiv.org/abs/2504.09858)

* **Unlocking efficient long-to-short LLM reasoning with model merging**  
  *Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan*  
  2025.  
  [ðŸ“„ arXiv:2503.20641](https://arxiv.org/abs/2503.20641)



---



## ðŸ”¹ Benchmarks

* **OptimalThinkingBench: Evaluating Over and Underthinking in LLMs**  
  *Pranjal Aggarwal, Seunghyun Kim, Julien Lanchantin, Sean Welleck, Jason Weston, Ilya Kulikov, Shubham Saha*  
  2025.  
  [ðŸ“„ arXiv:2508.13141](https://arxiv.org/abs/2508.13141)

---

## ðŸ”¹ Uncategorized

* **VeriThinker: Learning to verify makes reasoning model efficient**  
  *Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, Xinchao Wang*  
  2025.  
  [ðŸ“„ arXiv:2505.17941](https://arxiv.org/abs/2505.17941)



* **Do NOT think that much for 2+3=? On the overthinking of O1-like LLMs**  
  *Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*  
  2024.  
  [ðŸ“„ arXiv:2412.21187](https://arxiv.org/abs/2412.21187)

* **DAST: Difficulty-adaptive slow-thinking for large reasoning models**  
  *Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Shiguo Lian*  
  2025.  
  [ðŸ“„ arXiv:2503.04472](https://arxiv.org/abs/2503.04472)

* **O1-Pruner: Length-harmonizing fine-tuning for O1-like reasoning pruning**  
  *Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, Dacheng Tao*  
  2025.  
  [ðŸ“„ arXiv:2501.12570](https://arxiv.org/abs/2501.12570)

* **Training language models to reason efficiently**  
  *Daman Arora, Andrea Zanette*  
  2025.  
  [ðŸ“„ arXiv:2502.04463](https://arxiv.org/abs/2502.04463)

* **C3ot: Generating shorter chain-of-thought without compromising effectiveness**    
  *Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou*  
  2024.   
  [ðŸ“„ arXiv:2412.11664](https://arxiv.org/abs/2412.11664)  


---

## ðŸ”¹ How to Contribute

If you come across new papers on **efficient reasoning in LLMs**, feel free to open a pull request or issue with the details.

---
