# Awesome-Efficient-Reasoning-Papers

---

# ðŸ“š Efficient Reasoning in LLMs

This repository tracks research papers related to **efficient reasoning in large language models (LLMs)**.

---

## ðŸ”¹ Letting Model Learn When to Think (Hybrid-Reasoning)

* **AdaptThink: Reasoning models can learn when to think**
  *Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, Juanzi Li*, 2025.
  [ðŸ“„ arXiv:2505.13417](https://arxiv.org/abs/2505.13417)

* **ThinkLess: LLM learns when to think**
  *Gongfan Fang, Xiaotian Ma, Xinyu Wang*, 2025.
  [ðŸ“„ arXiv:2505.13379](https://arxiv.org/abs/2505.13379)

* **Think only when you need with large hybrid-reasoning models**
  *Linyang Jiang, Xiaoxuan Wu, Shilin Huang, Qi Dong, Zhiqiang Chi, Li Dong, Xiaodong Zhang, Tian Lv, Lijuan Cui, Furu Wei*, 2025.
  [ðŸ“„ arXiv:2505.14631](https://arxiv.org/abs/2505.14631)

---

## ðŸ”¹ Reducing Response Length

* **Reasoning models can be effective without thinking**
  *Weizhi Ma, Jinhyuk He, Charles Snell, Thomas Griggs, Sewon Min, Matei Zaharia*, 2025.
  [ðŸ“„ arXiv:2504.09858](https://arxiv.org/abs/2504.09858)

* **L1: Controlling how long a reasoning model thinks with reinforcement learning**
  *Pranjal Aggarwal, Sean Welleck*, 2025.
  [ðŸ“„ arXiv:2503.04697](https://arxiv.org/abs/2503.04697)

* **Unlocking efficient long-to-short LLM reasoning with model merging**
  *Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan*, 2025.
  [ðŸ“„ arXiv:2503.20641](https://arxiv.org/abs/2503.20641)

---

## ðŸ”¹ Routing

* **Arch-Router: Aligning LLM routing with human preferences**
  *Co Tran, Salman Paracha, Adil Hafeez, Shuguang Chen*, 2025.
  [ðŸ“„ arXiv:2506.16655](https://arxiv.org/abs/2506.16655)

---



## ðŸ”¹ Uncategorized

* **VeriThinker: Learning to verify makes reasoning model efficient**
  *Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, Xinchao Wang*, 2025.
  [ðŸ“„ arXiv:2505.17941](https://arxiv.org/abs/2505.17941)

* **OptimalThinkingBench: Evaluating Over and Underthinking in LLMs**
  *Pranjal Aggarwal, Seunghyun Kim, Julien Lanchantin, Sean Welleck, Jason Weston, Ilya Kulikov, Shubham Saha*, 2025.
  [ðŸ“„ arXiv:2508.13141](https://arxiv.org/abs/2508.13141)

* **Do NOT think that much for 2+3=? On the overthinking of O1-like LLMs**
  *Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*, 2024.
  [ðŸ“„ arXiv:2412.21187](https://arxiv.org/abs/2412.21187)

* **DAST: Difficulty-adaptive slow-thinking for large reasoning models**
  *Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Shiguo Lian*, 2025.
  [ðŸ“„ arXiv:2503.04472](https://arxiv.org/abs/2503.04472)

* **O1-Pruner: Length-harmonizing fine-tuning for O1-like reasoning pruning**
  *Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, Dacheng Tao*, 2025.
  [ðŸ“„ arXiv:2501.12570](https://arxiv.org/abs/2501.12570)

* **Training language models to reason efficiently**
  *Daman Arora, Andrea Zanette*, 2025.
  [ðŸ“„ arXiv:2502.04463](https://arxiv.org/abs/2502.04463)

* **Think when you need: Self-adaptive chain-of-thought learning**  
  *Jiaqi Yang, Kaixuan Lin, Xinyu Yu*, 2025.  
  [ðŸ“„ arXiv:2504.03234](https://arxiv.org/abs/2504.03234)


---

## ðŸ”¹ How to Contribute

If you come across new papers on **efficient reasoning in LLMs**, feel free to open a pull request or issue with the details.

---

